{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/input.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = [word.lower() for word in word_tokenize(text) if word.isidentifier()]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list = sent_tokenize(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662 68\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for sentence in sentences_list:\n",
    "    words = tokenize(sentence)\n",
    "    vocabulary.update(words)\n",
    " \n",
    "vocabulary = list(vocabulary)\n",
    "word_index = {w: idx for idx, w in enumerate(vocabulary)}\n",
    " \n",
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "DOCUMENTS_COUNT = len(sentences_list)\n",
    " \n",
    "print(VOCABULARY_SIZE, DOCUMENTS_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAKE_TOP = 7\n",
    "\n",
    "word_idf = defaultdict(lambda: 0)\n",
    "for sentence in sentences_list:\n",
    "    words = set(tokenize(sentence))\n",
    "    for word in words:\n",
    "        word_idf[word] += 1\n",
    "\n",
    "for word in vocabulary:\n",
    "    word_idf[word] = math.log(DOCUMENTS_COUNT / float(1 + word_idf[word]))\n",
    "def word_tf(word, document):\n",
    "    if isinstance(document, str):\n",
    "        document = tokenize(document)\n",
    "    return document.count(word) / len(document)\n",
    "   \n",
    "def tf_idf(word, document):\n",
    "    if isinstance(document, str):\n",
    "        document = tokenize(document)\n",
    "        \n",
    "    if not word in document:\n",
    "        return .0\n",
    "    \n",
    "    return word_tf(word, document) * word_idf[word]\n",
    "\n",
    "score = np.zeros(DOCUMENTS_COUNT)\n",
    "for index, sentence in enumerate(sentences_list):\n",
    "    score[index] = sum([tf_idf(word, sentence) for word in tokenize(sentence)])\n",
    "    \n",
    "    \n",
    "indexes = np.argpartition(score, -TAKE_TOP)[-TAKE_TOP:]\n",
    "\n",
    "summary = np.array(sentences_list)[indexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we’re focused on — and why we’re launching the token and the platform in lockstep — is that as much as possible the token should be valued based on its product value.\n",
      "\n",
      "In a blog post, Bustillos wrote: \"Imagine adding information to a news story you’re interested in, and not just getting thumbs up or ‘likes’ for it, but getting paid for it.\n",
      "\n",
      "Same with comments: make a comment that others find interesting, and your subscriber account can be credited with microtips that will accrue for as long as they continue to read and value your contribution.\"\n",
      "\n",
      "In November 2017, two New York–based sites specializing in local reportage, Gothamist (which also had outlets in other cities) and DNAinfo, were abruptly shut down by their owner, Joe Ricketts, a Trump-supporting billionaire, after their staff voted to unionize.\n",
      "\n",
      "Early on, according to co-founder and communications chief Matt Coolidge, the Civil team decided to focus on three types of journalism: local coverage, investigative reporting, and reporting on public policy.\n",
      "\n",
      "\"We are going to be facing pressures of industry groups and PACs and lobbyists, and as we dig into that, having the whole Civil community behind us and having our work archived on a blockchain is a really strong signal — the strongest possible technological signal of our editorial mission and our independence,\" Moore says.\n",
      "\n",
      "Either they fund their operations by putting their journalism behind a paywall — doable if you have a large, loyal audience like the New Yorker or the Times of London, but less of an option for smaller or local publications — or through advertising, where Google and Facebook greedily take the lion’s share of revenue for themselves.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
