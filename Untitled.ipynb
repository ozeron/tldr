{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/input.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = [word.lower() for word in word_tokenize(text) if word.isidentifier()]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list = sent_tokenize(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662 68\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for sentence in sentences_list:\n",
    "    words = tokenize(sentence)\n",
    "    vocabulary.update(words)\n",
    " \n",
    "vocabulary = list(vocabulary)\n",
    "word_index = {w: idx for idx, w in enumerate(vocabulary)}\n",
    " \n",
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "DOCUMENTS_COUNT = len(sentences_list)\n",
    " \n",
    "print(VOCABULARY_SIZE, DOCUMENTS_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAKE_TOP = 7\n",
    "\n",
    "word_idf = defaultdict(lambda: 0)\n",
    "for sentence in sentences_list:\n",
    "    words = set(tokenize(sentence))\n",
    "    for word in words:\n",
    "        word_idf[word] += 1\n",
    "\n",
    "for word in vocabulary:\n",
    "    word_idf[word] = math.log(DOCUMENTS_COUNT / float(1 + word_idf[word]))\n",
    "def word_tf(word, document):\n",
    "    if isinstance(document, str):\n",
    "        document = tokenize(document)\n",
    "    return document.count(word) / len(document)\n",
    "   \n",
    "def tf_idf(word, document):\n",
    "    if isinstance(document, str):\n",
    "        document = tokenize(document)\n",
    "        \n",
    "    if not word in document:\n",
    "        return .0\n",
    "    \n",
    "    return word_tf(word, document) * word_idf[word]\n",
    "\n",
    "score = np.zeros(DOCUMENTS_COUNT)\n",
    "for index, sentence in enumerate(sentences_list):\n",
    "    score[index] = sum([tf_idf(word, sentence) for word in tokenize(sentence)])\n",
    "    \n",
    "    \n",
    "indexes = np.argpartition(score, -TAKE_TOP)[-TAKE_TOP:]\n",
    "\n",
    "summary = np.array(sentences_list)[indexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What we’re focused on — and why we’re launching the token and the platform in lockstep — is that as much as possible the token should be valued based on its product value.\n",
      "\n",
      "In a blog post, Bustillos wrote: \"Imagine adding information to a news story you’re interested in, and not just getting thumbs up or ‘likes’ for it, but getting paid for it.\n",
      "\n",
      "Same with comments: make a comment that others find interesting, and your subscriber account can be credited with microtips that will accrue for as long as they continue to read and value your contribution.\"\n",
      "\n",
      "In November 2017, two New York–based sites specializing in local reportage, Gothamist (which also had outlets in other cities) and DNAinfo, were abruptly shut down by their owner, Joe Ricketts, a Trump-supporting billionaire, after their staff voted to unionize.\n",
      "\n",
      "Early on, according to co-founder and communications chief Matt Coolidge, the Civil team decided to focus on three types of journalism: local coverage, investigative reporting, and reporting on public policy.\n",
      "\n",
      "\"We are going to be facing pressures of industry groups and PACs and lobbyists, and as we dig into that, having the whole Civil community behind us and having our work archived on a blockchain is a really strong signal — the strongest possible technological signal of our editorial mission and our independence,\" Moore says.\n",
      "\n",
      "Either they fund their operations by putting their journalism behind a paywall — doable if you have a large, loyal audience like the New Yorker or the Times of London, but less of an option for smaller or local publications — or through advertising, where Google and Facebook greedily take the lion’s share of revenue for themselves.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.corpora'; 'gensim' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-802bdf6d312e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/tldr/gensim.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim.corpora'; 'gensim' is not a package"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6377ac929e40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "documents = [tokenize(sentence) for sentence in sentences_list]\n",
    "dictionary = Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-28fc604ae919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfModel' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_model = TfidfModel([dictionary.doc2bow(d) for d in documents], id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 0.07524574611981329,\n",
       " 15: 0.0995598939275996,\n",
       " 21: 0.09555481181378815,\n",
       " 26: 0.04340937044338817,\n",
       " 38: 0.0564532845555997,\n",
       " 58: 0.07506888991174153,\n",
       " 75: 0.11297830267846973,\n",
       " 184: 0.2212229146982923,\n",
       " 189: 0.2212229146982923,\n",
       " 192: 0.11801759835534606,\n",
       " 193: 0.14967711782369944,\n",
       " 205: 0.2436857012398502,\n",
       " 234: 0.167100608688381,\n",
       " 258: 0.2753452207082036,\n",
       " 375: 0.1895633952299389,\n",
       " 428: 0.2753452207082036,\n",
       " 657: 0.32946752671811486,\n",
       " 658: 0.32946752671811486,\n",
       " 659: 0.32946752671811486,\n",
       " 660: 0.32946752671811486,\n",
       " 661: 0.32946752671811486}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_values = dict(tfidf_model[dictionary.doc2bow(tokenize(sentence))])\n",
    "tfidf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\"Someone who’s really passionate about supporting one of our newsrooms but at this point is still only comfortable transacting with a credit card is more than capable of consuming the news and accessing this site,\" Iles says.',\n",
       "       'Iles says that launching their own token should mean that the volatility of ether\\u200a—\\u200athe Ethereum blockchain cryptocurrency whose value dropped by nearly 50 percent in January after rocketing up tenfold over 2017—should have a \"minimal impact\" on Civil.',\n",
       "       '\"We are going to be facing pressures of industry groups and PACs and lobbyists, and as we dig into that, having the whole Civil community behind us and having our work archived on a blockchain is a really strong signal\\u200a—\\u200athe strongest possible technological signal of our editorial mission and our independence,\" Moore says.',\n",
       "       'Billionaire-Proofing the News When Ricketts shuttered Gothamist and DNAinfo, and when Thiel’s lawsuit killed Gawker, there was a real danger that the crucial journalism all three sites did would not only cease, but be wiped from history entirely, their archives expunged.',\n",
       "       'The former director of the Participatory Politics Foundation, a nonprofit open-government group that operated OpenCongress.org\\u200a—\\u200aa site that tracked the revolving door between Congress and lobbying and became a leading resource for government transparency\\u200a—\\u200ais now part of a project with an ambitious goal: finding a way to save journalism.',\n",
       "       'Either they fund their operations by putting their journalism behind a paywall\\u200a—\\u200adoable if you have a large, loyal audience like the New Yorker or the Times of London, but less of an option for smaller or local publications\\u200a—\\u200aor through advertising, where Google and Facebook greedily take the lion’s share of revenue for themselves.',\n",
       "       'As it turns out, the solution to both crises facing journalism may come from the same technological root: blockchain, the technology that distributes data across a network of participating nodes using cryptographic proof and removes the necessity for information to be controlled by a central authority.'],\n",
       "      dtype='<U340')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = np.zeros(DOCUMENTS_COUNT)\n",
    "for index, sentence in enumerate(sentences_list):\n",
    "    tfidf_values = dict(tfidf_model[dictionary.doc2bow(tokenize(sentence))])\n",
    "    score[index] = sum(tfidf_values.values())\n",
    "    \n",
    "    \n",
    "indexes = np.argpartition(score, -TAKE_TOP)[-TAKE_TOP:]\n",
    "\n",
    "summary = np.array(sentences_list)[indexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08280056510807891,\n",
       " 0.11863953603372757,\n",
       " 0.10334534346003607,\n",
       " 0.16028911475528007,\n",
       " 0.128782424729828,\n",
       " 0.03150669002545206,\n",
       " 0.06576904467892389,\n",
       " 0.04166432631659493,\n",
       " 0.16028911475528007,\n",
       " 0.09192196035635852,\n",
       " 0.04198503814417171,\n",
       " 0.11863953603372757,\n",
       " 0.16028911475528007,\n",
       " 0.1418588825685453,\n",
       " 0.05562615598282344,\n",
       " 0.05340842124784926,\n",
       " 0.06301338005090412,\n",
       " 0.16028911475528007,\n",
       " 0.04166432631659493,\n",
       " 0.16028911475528007,\n",
       " 0.02411946595737138,\n",
       " 0.1418588825685453]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tf_idf(word, sentence) for word in tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "score = tfidf.transform(sentences_list).sum(axis=1).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.argpartition(score, -TAKE_TOP, axis=1)[-TAKE_TOP:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.argpartition(score, -TAKE_TOP, axis=1)[0, -TAKE_TOP:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['In November 2017, two New York–based sites specializing in local reportage, Gothamist (which also had outlets in other cities) and DNAinfo, were abruptly shut down by their owner, Joe Ricketts, a Trump-supporting billionaire, after their staff voted to unionize.',\n",
       "        'Billionaire-Proofing the News When Ricketts shuttered Gothamist and DNAinfo, and when Thiel’s lawsuit killed Gawker, there was a real danger that the crucial journalism all three sites did would not only cease, but be wiped from history entirely, their archives expunged.',\n",
       "        'The former director of the Participatory Politics Foundation, a nonprofit open-government group that operated OpenCongress.org\\u200a—\\u200aa site that tracked the revolving door between Congress and lobbying and became a leading resource for government transparency\\u200a—\\u200ais now part of a project with an ambitious goal: finding a way to save journalism.',\n",
       "        'Either they fund their operations by putting their journalism behind a paywall\\u200a—\\u200adoable if you have a large, loyal audience like the New Yorker or the Times of London, but less of an option for smaller or local publications\\u200a—\\u200aor through advertising, where Google and Facebook greedily take the lion’s share of revenue for themselves.',\n",
       "        'As it turns out, the solution to both crises facing journalism may come from the same technological root: blockchain, the technology that distributes data across a network of participating nodes using cryptographic proof and removes the necessity for information to be controlled by a central authority.',\n",
       "        'There are, in fact, two simultaneous existential crises facing journalism: dwindling public confidence and support for media in the face of the rise in fake news and clickbait, and collapsing advertising models that supported journalism in the past.',\n",
       "        '\"We are going to be facing pressures of industry groups and PACs and lobbyists, and as we dig into that, having the whole Civil community behind us and having our work archived on a blockchain is a really strong signal\\u200a—\\u200athe strongest possible technological signal of our editorial mission and our independence,\" Moore says.'],\n",
       "       dtype='<U340')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in np.array(sentences_list)[indexes]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
